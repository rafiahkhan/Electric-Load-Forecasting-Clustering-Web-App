{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ce94f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7879a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Paths & filenames ---\n",
    "DATA_DIR = r'/home/muhammad/Education/Data Mining Lab/Project/mining/json/data'\n",
    "WEATHER_FILES = [\n",
    "    'dallas.json','houston.json','la.json','nyc.json',\n",
    "    'san_diego.json','san_jose.json','san_antonio.json',\n",
    "    'phoenix.json','philadelphia.json','seattle.json'\n",
    "]\n",
    "DEMAND_FILES = {\n",
    "    'nyc': 'cleaned_subregion_data.csv',\n",
    "    'phoenix': 'cleaned_balance_data.csv',\n",
    "    'seattle': 'cleaned_balance_data.csv',\n",
    "    'houston': 'cleaned_texas_data.csv',\n",
    "    'san antonio': 'cleaned_texas_data.csv',\n",
    "    'dallas': 'cleaned_texas_data.csv'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf7fe879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Load & unify weather JSONs ---\n",
    "weather_dfs = []\n",
    "for fname in WEATHER_FILES:\n",
    "    city_key = os.path.splitext(fname)[0].replace('_', ' ').lower()\n",
    "    file_path = os.path.join(DATA_DIR, fname)\n",
    "    df = pd.read_json(file_path)\n",
    "    df['city'] = city_key\n",
    "    df['timestamp'] = pd.to_datetime(df['time'], unit='s')\n",
    "    df.drop(columns=['time'], inplace=True)\n",
    "    weather_dfs.append(df)\n",
    "weather_df = pd.concat(weather_dfs, ignore_index=True)\n",
    "weather_df.to_csv(os.path.join(DATA_DIR, 'weather_data.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70afc637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Load & unify demand sources ---\n",
    "demand_dfs = []\n",
    "for city, fname in DEMAND_FILES.items():\n",
    "    path = os.path.join(DATA_DIR, fname)\n",
    "    df = pd.read_csv(path)\n",
    "    # parse timestamp column\n",
    "    if 'local_time' in df.columns:\n",
    "        df['timestamp'] = pd.to_datetime(df['local_time'])\n",
    "    elif 'date' in df.columns:\n",
    "        df['timestamp'] = pd.to_datetime(df['date'])\n",
    "    # select & rename demand\n",
    "    df = df.rename(columns={'demand': 'demand_mwh'})\n",
    "    # ensure city column exists\n",
    "    if 'city' not in df.columns:\n",
    "        # texas file: pivot wide to long\n",
    "        df = df.melt(\n",
    "            id_vars=['timestamp'],\n",
    "            value_vars=[c for c in df.columns if c not in ['timestamp','date']],\n",
    "            var_name='city', value_name='demand_mwh'\n",
    "        )\n",
    "        df['city'] = df['city'].str.lower()\n",
    "    else:\n",
    "        df['city'] = df['city'].str.lower()\n",
    "    demand_dfs.append(df[['timestamp','city','demand_mwh']])\n",
    "demand_df = pd.concat(demand_dfs, ignore_index=True)\n",
    "demand_df.to_csv(os.path.join(DATA_DIR, 'demand_data.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42caa54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. Merge weather & demand ---\n",
    "combined_df = pd.merge(\n",
    "    weather_df,\n",
    "    demand_df,\n",
    "    on=['timestamp','city'],\n",
    "    how='left'\n",
    ")\n",
    "combined_df.to_csv(os.path.join(DATA_DIR, 'combined_data.csv'), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
